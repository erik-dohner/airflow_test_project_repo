{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:GCP credentials found in environment variable: C:\\Users\\edohner\\OneDrive - Lyric Opera of Chicago\\Desktop\\Python Learning\\airflow_test_project\\gcp_service_account.json\n",
      "INFO:root:GCP credentials JSON loaded successfully.\n",
      "INFO:root:Using project ID from JSON credentials: dbt-test-449821\n",
      "INFO:root:Retrieved normalized responses csv from blob: normalized_data.csv, project: dbt-test-449821\n",
      "INFO:root:Loading normalized responses to BigQuery, table: dbt-test-449821.pipeline.raw_sm_responses\n",
      "INFO:root:Job completed successfully with 56 rows loaded.\n"
     ]
    }
   ],
   "source": [
    "from api_library import StorageConnect\n",
    "from api_library import BQConnect\n",
    "from google.cloud import bigquery\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def bq_laod():\n",
    "    # Initiate the StorageConnect object\n",
    "    storage_conn = StorageConnect()\n",
    "\n",
    "    # Get normalized.csv\n",
    "    try:\n",
    "        normalized_string_data = storage_conn.sm_normalized_responses_blob.download_as_text()\n",
    "        if normalized_string_data:\n",
    "            logging.info(f'Retrieved normalized responses csv from blob: {storage_conn.sm_normalized_responses_blob.name}, project: {storage_conn.project_id}')\n",
    "        else: \n",
    "            raise RuntimeError('Downloaded file is empty')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to retrieve normalized responses CSV from blob: {storage_conn.sm_normalized_responses_blob.name}, project: {storage_conn.project_id}. Error: {e}\", exc_info=True)\n",
    "\n",
    "    # create dataframe\n",
    "    normalized_df = pd.read_csv(StringIO(normalized_string_data))\n",
    "\n",
    "    # Initiate the BigQueryConnect object\n",
    "    bq_conn = BQConnect()\n",
    "\n",
    "    # Fixing datatypes\n",
    "    normalized_df['date'] = pd.to_datetime(normalized_df['date'], errors='coerce')\n",
    "\n",
    "    str_columns = [ \"choice_id\", \"row_id\", \"choice_metadata.weight\", \"question_id\",\n",
    "        \"respondent_id\", \"collector_id\", \"survey_id\", \"const_id\",\n",
    "        \"email\", \"performance_code\", \"production_name\", \"tag_data\",\n",
    "        \"text\", \"other_id\"]\n",
    "\n",
    "    normalized_df[str_columns] = normalized_df[str_columns].astype(str)\n",
    "\n",
    "    # define project_id, dataset_id and table_id\n",
    "    dataset_id = bq_conn.dataset_pipeline\n",
    "    table_id = f'{dataset_id}.raw_sm_responses'\n",
    "\n",
    "    # renaming columns\n",
    "    normalized_df.rename(columns={\n",
    "        'choice_metadata.weight': 'choice_metadata_weight'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # configure the specific table we're sending the df to in bigquery\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"choice_id\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"row_id\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"choice_metadata_weight\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"question_id\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"respondent_id\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"collector_id\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"survey_id\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"date\", \"TIMESTAMP\"),\n",
    "            bigquery.SchemaField(\"const_id\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"email\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"performance_code\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"production_name\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"tag_data\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"text\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"other_id\", \"STRING\"),\n",
    "        ],\n",
    "        write_disposition=\"WRITE_TRUNCATE\"\n",
    "    )\n",
    "\n",
    "    # load the table to bigquery\n",
    "    load_job = bq_conn.client.load_table_from_dataframe(\n",
    "        normalized_df, table_id, job_config=job_config\n",
    "    )\n",
    "\n",
    "    logging.info(f'Loading normalized responses to BigQuery, table: {table_id}')\n",
    "\n",
    "    # wait for the result\n",
    "    load_job.result()  \n",
    "\n",
    "    # Check job status and log\n",
    "    if load_job.state == 'DONE':\n",
    "        if load_job.error_result:\n",
    "            logging.error(f\"Job failed with error: {load_job.error_result}\")\n",
    "        else:\n",
    "            logging.info(f\"Job completed successfully with {load_job.output_rows} rows loaded.\")\n",
    "    else:\n",
    "        logging.warning(f\"Job state: {load_job.state}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "bq_laod()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
